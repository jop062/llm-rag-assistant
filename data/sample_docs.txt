RAG (Retrieval-Augmented Generation) is a technique that improves LLM responses by retrieving relevant documents and injecting them as context into the prompt. It reduces hallucinations by grounding the model in source material.

A common RAG pipeline includes: chunking documents, creating embeddings, indexing embeddings in a vector database, retrieving top-k relevant chunks for a user query, and generating an answer using only those chunks.